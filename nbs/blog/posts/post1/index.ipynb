{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Lesson 3: HuggingFace NLP Models\"\n",
    "author: \"Chris Kroenke\"\n",
    "date: \"10/07/2023\"\n",
    "toc: true \n",
    "badges: true\n",
    "categories: [fractal, python, LLM]\n",
    "image: llama.jpeg\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Running powerful NLP models with the HuggingFace `transformers` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the third lesson of the course. Let's recap our progress so far:  \n",
    "\n",
    "- Lesson 1: We made a python environment for LLMs.  \n",
    "- Lesson 2: Set up a personal blog to track our progress.\n",
    "\n",
    "Next we will use our first LLM. We'll start with a Natural Language Processing (NLP) model provided by the HuggingFace team.\n",
    "\n",
    "## Notebook best practices  \n",
    "\n",
    "First, let's set up our notebook to be fully interactive and easy to use. We can do this with a couple of \"magic functions\" built-in to Jupyter.  \n",
    "\n",
    "Specifically, we use the magic `autoreload` and `matplotlib` functions. The cell below shows them in action:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best practice notebook magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what these magic functions do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`autoreload` dynamically reloads code libraries, even as they're changing under the hood. That means we do not have to restart the notebook after every change. We can instead code and experiment on the fly.   \n",
    "\n",
    "`matplotlib inline` automatically displays any plots below the code cell that created them. The plots are also saved in the notebook itself, which is perfect for our blog posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our notebooks going forward will start with these magic functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the `\"hello, world!\"` of NLP: sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::: callout-note\n",
    "The code and examples below are based on the official HuggingFace tutorial, reworked to better suit the course.\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we're selling some product. And we've gathered a bunch of reviews from a large group of users to find out both the good and bad things that people are saying. The bad reviews will point out where our product needs improving. Positive reviews will show what we're doing right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figuring out the tone of a statement (*positive vs. negative*) is an area of NLP known as `sentiment analysis`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through each review would give us a ton of insight about our product. But, it would take a ton of intense and manual effort. Enter Machine Learning to the rescue! An NLP model can automatically analyze and classify the reviews in bulk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the HuggingFace NLP model that we'll run. At a high level, the model is built around three key pieces:  \n",
    "\n",
    "1. A `Config` file.  \n",
    "2. A `Preprocessor` file.   \n",
    "3. `Model` file(s).   \n",
    "\n",
    "The HuggingFace API has a handy, high-level `pipeline` that wraps up all three objects for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::: callout-important\n",
    "Before going forward, make sure that the `llm-env` environment from the first lesson is active. This environment has the HuggingFace libraries used below.\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses the `transformers` library to build a Sentiment Analysis `pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the pipeline object from HuggingFace\n",
    "from transformers import pipeline\n",
    "\n",
    "# create a sentiment analysis pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                                                                                                                                                                        Since we didn't specify a model, you can see in the output above that HuggingFace picked a [distilbert model](distilbert-base-uncased-finetuned-sst-2-english) for us by default.  \n",
    "\n",
    "We will learn more about what exactly `distilbert` is and how it works later on. For now, think of it as a useful NLP genie who can look at a sentence and tell us whether its has a positive or negative tone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's find out what the model thinks about the sentence: `\"HuggingFace pipelines are awesome!\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.4380775988101959}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment analysis on a simple, example sentence\n",
    "example_sentence = \"Si, Me Gusta AI!\"\n",
    "classifier(example_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. We see a strong confident score for a `POSITIVE` label, as could be expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pass many sentences at once, which starts to show the bulk processing power of these models. Let's process four sentences at once: three positive ones, and a clearly negative one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1 star, with score: 0.2874\n",
      "label: 5 stars, with score: 0.626\n",
      "label: 5 stars, with score: 0.33\n",
      "label: 5 stars, with score: 0.6003\n"
     ]
    }
   ],
   "source": [
    "# many sentences at once, in a python list\n",
    "many_sentences = [\n",
    "    \"A chicken is a chicken.\",\n",
    "    \"Everybody knows that!\",\n",
    "    \"A rose is a rose is a rose.\",\n",
    "    \"I really like this course so far\",\n",
    "]\n",
    "\n",
    "# process many sentences at once\n",
    "\n",
    "results = classifier(many_sentences)\n",
    "\n",
    "# check the tone of each sentence\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You've now ran a HuggingFace pipeline and used it to analyze the tone of a few sentences. Next, let's take a closer look at the pipeline object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going inside the `pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, a pipeline handles three key HuggingFace NLP pieces: Config, Preprocessor, and Model.\n",
    "\n",
    "To better understand each piece, let's take one small step down the ladder of abstraction and build our own simple pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same `distilbert` model from before. First we need the three key pieces mentioned above. Thankfully, we can import each of these pieces from the `transformers` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `config` class is a simple map with the options and configurations of a model. It has the key-value pairs that define a model's architecture and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config for the model\n",
    "from transformers import DistilBertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `preprocessor` object in this case is a `Tokenizer`. Tokenizers convert strings and characters into special tensor inputs for the LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::: callout-note\n",
    "Correctly pre-processing inputs is one of the most important and error-prone steps in using ML models. In other words, it's good to offload to a class that's already been tested and debugged.\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input preprocessor to tokenize strings\n",
    "from transformers import DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model` class holds the weights and parameters for the actual LLM. It's the \"meat and bones\" of the setup, so to speak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the text classifier model\n",
    "from transformers import DistilBertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know a model's full, proper name in to load it from HuggingFace. Its name is how we find the model on the [HuggingFace Model Hub](https://huggingface.co/docs/hub/models-the-hub).  \n",
    "\n",
    "Once we know its full name, there is a handy `from_pretrained()` function that will automatically find and download the pieces for us.  \n",
    "\n",
    "In this case, the distilbert model's full name is:  \n",
    "> `distilbert-base-uncased-finetuned-sst-2-english`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis model name\n",
    "model_name = 't5-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below we can now load each of the three NLP pieces for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the config\n",
    "config = DistilBertConfig.from_pretrained(model_name)\n",
    "\n",
    "# create the input tokenizer \n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# create the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will compose these three pieces together to mimic the original `pipeline` example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting together a `simple_pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a `preprocess` function to turn a given `text` string into the proper, tokenized inputs than an LLM expects.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text: str):\n",
    "    \"\"\"\n",
    "    Sends `text` through the model's tokenizer.  \n",
    "    The tokenizer turns words and characters into proper inputs for an NLP model.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(text, return_tensors='pt')\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this preprocessing function on the example sentence from earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 4205, 2018, 1037, 2307, 2154,  999,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually preprocessing the example sentence: \"HuggingFace pipelines are awesome!\"\n",
    "preprocess(example_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turned an input string into numerical embeddings for the LLM. We'll breakdown what exactly this output means later on in the course. For now, think of it as sanitizing and formatting the text into a format that the LLM has been trained to work with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, let's make our own `forward` function that run the LLM on preprocessed inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(text: str):\n",
    "    \"\"\"\n",
    "    First we preprocess the `text` into tokens.\n",
    "    Then we send the `tokenized_inputs` to the model.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = preprocess(text)\n",
    "    outputs = model(**tokenized_inputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m     \n",
      "\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext_pair\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext_pair_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTruncationStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_offsets_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           DistilBertTokenizerFast\n",
      "\u001b[0;31mString form:\u001b[0m   \n",
      "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased-finetuned-sst-2-english', vocab_siz <...> Token(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "           }\n",
      "\u001b[0;31mLength:\u001b[0m         30522\n",
      "\u001b[0;31mFile:\u001b[0m           ~/mambaforge/envs/llm-env/lib/python3.11/site-packages/transformers/models/distilbert/tokenization_distilbert_fast.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Construct a \"fast\" DistilBERT tokenizer (backed by HuggingFace's *tokenizers* library). Based on WordPiece.\n",
      "\n",
      "This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n",
      "refer to this superclass for more information regarding those methods.\n",
      "\n",
      "Args:\n",
      "    vocab_file (`str`):\n",
      "        File containing the vocabulary.\n",
      "    do_lower_case (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to lowercase the input when tokenizing.\n",
      "    unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n",
      "        The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
      "        token instead.\n",
      "    sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n",
      "        The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n",
      "        sequence classification or for a text and a question for question answering. It is also used as the last\n",
      "        token of a sequence built with special tokens.\n",
      "    pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n",
      "        The token used for padding, for example when batching sequences of different lengths.\n",
      "    cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n",
      "        The classifier token which is used when doing sequence classification (classification of the whole sequence\n",
      "        instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
      "    mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n",
      "        The token used for masking values. This is the token used when training this model with masked language\n",
      "        modeling. This is the token which the model will try to predict.\n",
      "    clean_text (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to clean the text before tokenization by removing any control characters and replacing all\n",
      "        whitespaces by the classic one.\n",
      "    tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see [this\n",
      "        issue](https://github.com/huggingface/transformers/issues/328)).\n",
      "    strip_accents (`bool`, *optional*):\n",
      "        Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n",
      "        value for `lowercase` (as in the original BERT).\n",
      "    wordpieces_prefix (`str`, *optional*, defaults to `\"##\"`):\n",
      "        The prefix for subwords.\n",
      "\u001b[0;31mCall docstring:\u001b[0m\n",
      "Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      "sequences.\n",
      "\n",
      "Args:\n",
      "    text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "    text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "    text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "    text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "\n",
      "    add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
      "        `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
      "        automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
      "        automatically.\n",
      "    padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      "        Activates and controls padding. Accepts the following values:\n",
      "\n",
      "        - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      "          sequence if provided).\n",
      "        - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      "          acceptable input length for the model if that argument is not provided.\n",
      "        - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      "          lengths).\n",
      "    truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      "        Activates and controls truncation. Accepts the following values:\n",
      "\n",
      "        - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      "          to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      "          truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      "          sequences (or a batch of pairs) is provided.\n",
      "        - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "          truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "        - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "          truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "        - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      "          greater than the model maximum admissible input size).\n",
      "    max_length (`int`, *optional*):\n",
      "        Controls the maximum length to use by one of the truncation/padding parameters.\n",
      "\n",
      "        If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      "        is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      "        length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      "    stride (`int`, *optional*, defaults to 0):\n",
      "        If set to a number along with `max_length`, the overflowing tokens returned when\n",
      "        `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      "        returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      "        argument defines the number of overlapping tokens.\n",
      "    is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      "        tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      "        which it will tokenize. This is useful for NER or token classification.\n",
      "    pad_to_multiple_of (`int`, *optional*):\n",
      "        If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      "        This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      "        `>= 7.5` (Volta).\n",
      "    return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      "        If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      "\n",
      "        - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      "        - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      "        - `'np'`: Return Numpy `np.ndarray` objects.\n",
      "\n",
      "    return_token_type_ids (`bool`, *optional*):\n",
      "        Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      "        the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      "\n",
      "        [What are token type IDs?](../glossary#token-type-ids)\n",
      "    return_attention_mask (`bool`, *optional*):\n",
      "        Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      "        to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      "\n",
      "        [What are attention masks?](../glossary#attention-mask)\n",
      "    return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      "        of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      "        of returning overflowing tokens.\n",
      "    return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return special tokens mask information.\n",
      "    return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return `(char_start, char_end)` for each token.\n",
      "\n",
      "        This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      "        Python's tokenizer, this method will raise `NotImplementedError`.\n",
      "    return_length  (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return the lengths of the encoded inputs.\n",
      "    verbose (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to print more information and warnings.\n",
      "    **kwargs: passed to the `self.tokenize()` method\n",
      "\n",
      "Return:\n",
      "    [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      "\n",
      "    - **input_ids** -- List of token ids to be fed to a model.\n",
      "\n",
      "      [What are input IDs?](../glossary#input-ids)\n",
      "\n",
      "    - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      "      if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      "\n",
      "      [What are token type IDs?](../glossary#token-type-ids)\n",
      "\n",
      "    - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      "      `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      "\n",
      "      [What are attention masks?](../glossary#attention-mask)\n",
      "\n",
      "    - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      "      `return_overflowing_tokens=True`).\n",
      "    - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      "      `return_overflowing_tokens=True`).\n",
      "    - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      "      regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      "    - **length** -- The length of the inputs (when `return_length=True`)"
     ]
    }
   ],
   "source": [
    "tokenizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what this outputs for our running example sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-4.2644,  4.6256]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = forward(example_sentence); outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see a lot going on in the `SequenceClassifierOutput` above. To be honest, this is where the original `pipeline` does most of the heavy-lifting for us. It takes the raw, detailed output from an LLM and converts it into a more human-readable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll mimic this heavy-lifting by using the `Config` class and model outputs to find out whether the sentence is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_outputs(outs):\n",
    "    \"\"\"\n",
    "    Converting the raw model outputs into a human-readable result.\n",
    "\n",
    "    Steps:\n",
    "        1. Grab the raw \"scores\" from the model for Positive and Negative labels.  \n",
    "        2. Find out which score is the highest (aka the model's decision).  \n",
    "        3. Use the `config` object to find the class label for the highest score.  \n",
    "        4. Turn the raw score into a human-readable probability value.  \n",
    "        5. Print out the predicted labels with its probability.  \n",
    "    \"\"\"\n",
    "    # 1. Grab the raw \"scores\" that from the model for Positive and Negative labels\n",
    "    logits = outs.logits\n",
    "\n",
    "    # 2. Find the strongest label score, aka the model's decision\n",
    "    pred_idx = logits.argmax(1).item()\n",
    "\n",
    "    # 3. Use the `config` object to find the class label\n",
    "    pred_label = config.id2label[pred_idx]  \n",
    "\n",
    "    # 4. Calculate the human-readable number for the score\n",
    "    pred_score = logits.softmax(-1)[:, pred_idx].item()\n",
    "\n",
    "    # 5. return the label and score in a dictionary\n",
    "    return {\n",
    "        'label': pred_label,\n",
    "        'score': pred_score, \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now put together a `simple_pipeline`, and check how it compares to the original `pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_pipeline(text):\n",
    "    \"\"\"\n",
    "    Putting the NLP pieces and functions together into a pipeline.\n",
    "    \"\"\"\n",
    "    # get the model's raw output\n",
    "    model_outs = forward(text)\n",
    "    # convert the raw outputs into a human readable result\n",
    "    predictions = process_outputs(model_outs)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the `simple_pipeline` on the example sentence, drumroll please..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'POSITIVE', 'score': 0.9883008599281311}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running out simple pipeline on the example text\n",
    "simple_pipeline(\"HIIII\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that, we too a small peek under the `pipeline` hood and built our own, simple working version.  \n",
    "\n",
    "One pain point: we had to know the full, proper name of the different `Distilbert*` pieces to import the Config, Preprocessor, and Model. This gets overwhelming fast given the flood of LLM models released almost daily. Thankfully, HuggingFace has come up with a great solution to this problem: the `Auto` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True HuggingFace magic: `Auto` classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `Auto` classes, we don't have to know the exact or proper name of the LLM's objects to import them. We only need the proper name of the model on the hub: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# viewing our distilbert model's name\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_name\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
     ]
    }
   ],
   "source": [
    "# viewing our distilbert model's name\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to import the Auto classes. Then we'll use them with the model name to create an even cleaner `simple_pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the Auto classes\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the three key NLP pieces with the Auto classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the pieces with `Auto` classes\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these pieces to build a `simple_pipeline` class that's cleaner than before, and can handle any model_name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentPipeline:\n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"\n",
    "        Simple Sentiment Analysis pipeline.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.config = AutoConfig.from_pretrained(self.model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)\n",
    "\n",
    "    def preprocess(self, text: str):\n",
    "        \"\"\"\n",
    "        Sends `text` through the LLM's tokenizer.  \n",
    "        The tokenizer turns words and characters into special inputs for the LLM.\n",
    "        \"\"\"\n",
    "        tokenized_inputs = self.tokenizer(text, return_tensors='pt')\n",
    "        return tokenized_inputs\n",
    "\n",
    "    def forward(self, text: str):\n",
    "        \"\"\"\n",
    "        First we preprocess the `text` into tokens.\n",
    "        Then we send the `token_inputs` to the model.\n",
    "        \"\"\"\n",
    "        token_inputs = self.preprocess(text)\n",
    "        outputs = self.model(**token_inputs)\n",
    "        return outputs\n",
    "\n",
    "    def process_outputs(self, outs):\n",
    "        \"\"\"\n",
    "        Here we mimic the post-processing that HuggingFace automatically does in its `pipeline`.  \n",
    "        \"\"\"\n",
    "        # grab the raw scores from the model for Positive and Negative labels\n",
    "        logits = outs.logits\n",
    "\n",
    "        # find the strongest label score, aka the model's decision\n",
    "        pred_idx = logits.argmax(1).item()\n",
    "\n",
    "        # use the `config` object to find the actual class label\n",
    "        pred_label = self.config.id2label[pred_idx]  \n",
    "\n",
    "        # calculate the human-readable probability score for this class\n",
    "        pred_score = logits.softmax(-1)[:, pred_idx].item()\n",
    "\n",
    "        # return the predicted label and its score\n",
    "        return {\n",
    "            'label': pred_label,\n",
    "            'score': pred_score, \n",
    "        }\n",
    "    \n",
    "    def __call__(self, text: str):\n",
    "        \"\"\"\n",
    "        Overriding the call method to easily and intuitively call the pipeline.\n",
    "        \"\"\"\n",
    "        model_outs = self.forward(text)\n",
    "        preds = self.process_outputs(model_outs)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the custom `SentimentPipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's leverage both the new class and a different model, to show the power of Auto classes.  \n",
    "\n",
    "For fun, let's use BERT model that was trained specifically on tweets. The full model's name is [`finiteautomata/bertweet-base-sentiment-analysis`](https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a different model\n",
    "new_model_name = 'finiteautomata/bert-base-spanish-wwm-uncased-reranker-25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        str\n",
      "\u001b[0;31mString form:\u001b[0m finiteautomata/bert-base-spanish-wwm-uncased-reranker-25\n",
      "\u001b[0;31mLength:\u001b[0m      56\n",
      "\u001b[0;31mDocstring:\u001b[0m  \n",
      "str(object='') -> str\n",
      "str(bytes_or_buffer[, encoding[, errors]]) -> str\n",
      "\n",
      "Create a new string object from the given object. If encoding or\n",
      "errors is specified, then the object must expose a data buffer\n",
      "that will be decoded using the given encoding and error handler.\n",
      "Otherwise, returns the result of object.__str__() (if defined)\n",
      "or repr(object).\n",
      "encoding defaults to sys.getdefaultencoding().\n",
      "errors defaults to 'strict'."
     ]
    }
   ],
   "source": [
    "new_model_name??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run it on our handy example sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence2 = \"Te odio!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'LABEL_1', 'score': 0.5055498480796814}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling our new, flexible pipeline\n",
    "simple_pipeline(example_sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_pipeline.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You've now built a flexible pipeline for Sentiment Analysis that can leverage most NLP models on the HuggingFace hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m TextSummarizationPipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first 4 points1 maximize the beauty - fully channel the beauty with in. Maybe ask what makes this moment beautiful? See if beauty can be increased in every situation. MtB also could be taken as A use of reason and also a disciplining of the senses to focus on beauty (i.e. all the pretty flowers, all the pretty birds).2 full expression - it takes a lot of effort for one to understand who they are when they are comfortable and how to channel the most real expressions of themselves What holds people back? Shyness, distraction (inability to focus on that which they want express)3 expect rising - this means our expectations are constantly rising. Kinda in line with give em an inch they\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll take a mile also related to law of diminishing returns -The law of diminishing returns states that in all productive processes, adding more of one factor of production, while holding all others constant (ceteris paribus), will at some point yield lower incremental per-unit returns. Tooo much ice cream too much cash4 power of pettiness - is the idea that pettiness is the destroyer of all people. That even the best of us cant truly be unaffected by the petty bullshit around us. They can however minimize its effects. In my own life I am obliterated by friends and coworkers snide remarks and judgments. How do I minimize its effects? By talking myself down, deep breaths, weed, alchohol, revenge...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 60\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43msummarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "Cell \u001b[0;32mIn[94], line 51\u001b[0m, in \u001b[0;36mTextSummarizationPipeline.__call__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Summarizes the provided text.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     preprocessed_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(preprocessed_text)\n\u001b[1;32m     53\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_outputs(model_outputs)\n",
      "Cell \u001b[0;32mIn[94], line 19\u001b[0m, in \u001b[0;36mTextSummarizationPipeline.preprocess\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     17\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Add an \"end of summary\" token if necessary for your model\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_end_of_summary_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "Cell \u001b[0;32mIn[94], line 28\u001b[0m, in \u001b[0;36mTextSummarizationPipeline.add_end_of_summary_token\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Modify this based on your model's specific end of summary token ID\u001b[39;00m\n\u001b[1;32m     27\u001b[0m eos_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[0;32m---> 28\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcat((input_ids, torch\u001b[38;5;241m.\u001b[39mtensor([eos_token_id])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m input_ids\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "class TextSummarizationPipeline:\n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"\n",
    "        Simple Text Summarization pipeline.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.config = AutoConfig.from_pretrained(self.model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)\n",
    "\n",
    "    def preprocess(self, text: str):\n",
    "        \"\"\"\n",
    "        Preprocesses text for summarization.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        # Add an \"end of summary\" token if necessary for your model\n",
    "        inputs['input_ids'] = self.add_end_of_summary_token(inputs['input_ids'])\n",
    "        return inputs\n",
    "\n",
    "    def add_end_of_summary_token(self, input_ids):\n",
    "        \"\"\"\n",
    "        Adds the end of summary token to the input sequence.\n",
    "        \"\"\"\n",
    "        # Modify this based on your model's specific end of summary token ID\n",
    "        eos_token_id = self.tokenizer.eos_token_id\n",
    "        input_ids = torch.cat((input_ids, torch.tensor([eos_token_id]).unsqueeze(0)), dim=-1)\n",
    "        return input_ids\n",
    "\n",
    "    def forward(self, preprocessed_text):\n",
    "        \"\"\"\n",
    "        Passes preprocessed text through the summarization model.\n",
    "        \"\"\"\n",
    "        # Include preprocessed_text['input_ids'] as decoder_input_ids\n",
    "        outputs = self.model(**preprocessed_text, decoder_input_ids=preprocessed_text['input_ids'])\n",
    "        return outputs\n",
    "\n",
    "    def process_outputs(self, model_outputs):\n",
    "        \"\"\"\n",
    "        Extracts the generated summary text.\n",
    "        \"\"\"\n",
    "        summary_ids = model_outputs.logits.argmax(-1)  # Get most likely token IDs\n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)  # Decode and remove special tokens\n",
    "        return summary\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        \"\"\"\n",
    "        Summarizes the provided text.\n",
    "        \"\"\"\n",
    "        preprocessed_text = self.preprocess(text)\n",
    "        model_outputs = self.forward(preprocessed_text)\n",
    "        summary = self.process_outputs(model_outputs)\n",
    "        return summary\n",
    "\n",
    "\n",
    "# Example usage\n",
    "summarizer = TextSummarizationPipeline(\"t5-base\")\n",
    "text = \"The first 4 points1 maximize the beauty - fully channel the beauty with in. Maybe ask what makes this moment beautiful? See if beauty can be increased in every situation. MtB also could be taken as A use of reason and also a disciplining of the senses to focus on beauty (i.e. all the pretty flowers, all the pretty birds).2 full expression - it takes a lot of effort for one to understand who they are when they are comfortable and how to channel the most real expressions of themselves What holds people back? Shyness, distraction (inability to focus on that which they want express)3 expect rising - this means our expectations are constantly rising. Kinda in line with give em an inch they'll take a mile also related to law of diminishing returns -The law of diminishing returns states that in all productive processes, adding more of one factor of production, while holding all others constant (ceteris paribus), will at some point yield lower incremental per-unit returns. Tooo much ice cream too much cash4 power of pettiness - is the idea that pettiness is the destroyer of all people. That even the best of us cant truly be unaffected by the petty bullshit around us. They can however minimize its effects. In my own life I am obliterated by friends and coworkers snide remarks and judgments. How do I minimize its effects? By talking myself down, deep breaths, weed, alchohol, revenge...\"\n",
    "summary = summarizer(text)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_name\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
     ]
    }
   ],
   "source": [
    "model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'maximize the beauty - fully channel the beauty with in . also could be taken as a use of reason and also a disciplining'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-base\")\n",
    "\n",
    "# Example usage\n",
    "text = \" maximize the beauty - fully channel the beauty with in. Maybe ask what makes this moment beautiful? See if beauty can be increased in every situation. MtB also could be taken as A use of reason and also a disciplining of the senses to focus on beauty (i.e. all the pretty flowers, all the pretty birds)\"\n",
    "\n",
    "summary = summarizer(text, max_length=30)\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conlusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook went through the basics of using a HuggingFace pipeline to run sentiment analysis on a few sentences. We then looked under the hood at the pipeline's three key pieces: Config, Preprocessor, and Model.   \n",
    "\n",
    "Lastly, we built our own `simple_pipeline` from scratch to see how the pieces fit together.\n",
    "\n",
    "The goal of this notebook was two fold. First, we wanted to gain hands-on experience with using the `transformers` API from HuggingFace. It's an incredibly powerful library, that lets us do what used to be difficult, research-level NLP tasks in a few lines of code.  \n",
    "\n",
    "Second, we wanted to get some familiarity with downloading models. The model weights that we downloaded from HuggingFace are the same ones that we will be fine-tuning, quantizing, and deploying on our devices throughout the course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two appendixes below. The first one gives a handy way of counting the number of weights in a model. The second one goes into more details about how to interactively debug an analyze the code in a Jupyter notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 1: Counting the number of parameters in a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet counts the number of trainable parameters in a model. It's a question that comes up often when working with LLMs, and having a quick reference to find out a rough model's size often comes in handy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Counts the number of trainable parameters in a `model`.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use it to count the number of parameters in the distilbert model from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Number of trainable params: 66,955,010'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the number of parameters in the last model used\n",
    "f\"Number of trainable params: {count_parameters(model):,}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 2: Inspecting the `classifier`, notebook style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the `classifier` object, exactly? Jupyter has many powerful ways of inspecting and analyzing its code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest ways of checking an object is to call it by itself in a code cell, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x1282d2290>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the contents of the `classifier` object\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the `classifier` is a type of `TextClassification` pipeline. This makes sense: we fed it an input sentence and asked it to classify the statement as *positive* vs. *negative*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a tab-autocomplete feature to find the members and methods of an object. For example, to look up everything in `classifier`, hit tab after adding a `.`.  \n",
    "\n",
    "Uncomment the cells below and hit the tab key to test the auto-complete feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x1282d2290>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tab after the `.` to auto-complete all variables/methods\n",
    "classifier.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you vaguely remember the name of a variable or function, say for example the `forward()` method. In that case you can type the first few letters and hit tab to auto-complete the full set of options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tab after the `.for` to auto-complete the rest of the options\n",
    "# classifier.for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking questions: `?` and `??`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can literally interrogate an object in Jupyter for more information.  \n",
    "\n",
    "If we tag a single `?` after an object, we'll get its basic documentation (docstring). Note that we omit it here to keep the notebook from getting too busy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           TextClassificationPipeline\n",
      "\u001b[0;31mString form:\u001b[0m    <transformers.pipelines.text_classification.TextClassificationPipeline object at 0x11cd918d0>\n",
      "\u001b[0;31mFile:\u001b[0m           ~/mambaforge/envs/llm-env/lib/python3.11/site-packages/transformers/pipelines/text_classification.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Text classification pipeline using any `ModelForSequenceClassification`. See the [sequence classification\n",
      "examples](../task_summary#sequence-classification) for more information.\n",
      "\n",
      "Example:\n",
      "\n",
      "```python\n",
      ">>> from transformers import pipeline\n",
      "\n",
      ">>> classifier = pipeline(model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
      ">>> classifier(\"This movie is disgustingly good !\")\n",
      "[{'label': 'POSITIVE', 'score': 1.0}]\n",
      "\n",
      ">>> classifier(\"Director tried too much.\")\n",
      "[{'label': 'NEGATIVE', 'score': 0.996}]\n",
      "```\n",
      "\n",
      "Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
      "\n",
      "This text classification pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
      "`\"sentiment-analysis\"` (for classifying sequences according to positive or negative sentiments).\n",
      "\n",
      "If multiple classification labels are available (`model.config.num_labels >= 2`), the pipeline will run a softmax\n",
      "over the results. If there is a single label, the pipeline will run a sigmoid over the result.\n",
      "\n",
      "The models that this pipeline can use are models that have been fine-tuned on a sequence classification task. See\n",
      "the up-to-date list of available models on\n",
      "[huggingface.co/models](https://huggingface.co/models?filter=text-classification).\n",
      "\n",
      "Arguments:\n",
      "    model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      "        The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      "        [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      "    tokenizer ([`PreTrainedTokenizer`]):\n",
      "        The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      "        [`PreTrainedTokenizer`].\n",
      "    modelcard (`str` or [`ModelCard`], *optional*):\n",
      "        Model card attributed to the model for this pipeline.\n",
      "    framework (`str`, *optional*):\n",
      "        The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      "        installed.\n",
      "\n",
      "        If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      "        both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      "        provided.\n",
      "    task (`str`, defaults to `\"\"`):\n",
      "        A task-identifier for the pipeline.\n",
      "    num_workers (`int`, *optional*, defaults to 8):\n",
      "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
      "        workers to be used.\n",
      "    batch_size (`int`, *optional*, defaults to 1):\n",
      "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
      "        the batch to use, for inference this is not always beneficial, please read [Batching with\n",
      "        pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
      "    args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
      "        Reference to the object in charge of parsing supplied pipeline parameters.\n",
      "    device (`int`, *optional*, defaults to -1):\n",
      "        Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
      "        the associated CUDA device id. You can pass native `torch.device` or a `str` too\n",
      "    torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      "        Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      "        (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`)\n",
      "    binary_output (`bool`, *optional*, defaults to `False`):\n",
      "        Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n",
      "        the raw output data e.g. text.\n",
      "    return_all_scores (`bool`, *optional*, defaults to `False`):\n",
      "        Whether to return all prediction scores or just the one of the predicted class.\n",
      "    function_to_apply (`str`, *optional*, defaults to `\"default\"`):\n",
      "        The function to apply to the model outputs in order to retrieve the scores. Accepts four different values:\n",
      "\n",
      "        - `\"default\"`: if the model has a single label, will apply the sigmoid function on the output. If the model\n",
      "          has several labels, will apply the softmax function on the output.\n",
      "        - `\"sigmoid\"`: Applies the sigmoid function on the output.\n",
      "        - `\"softmax\"`: Applies the softmax function on the output.\n",
      "        - `\"none\"`: Does not apply any function on the output.\n",
      "\u001b[0;31mCall docstring:\u001b[0m\n",
      "Classify the text(s) given as inputs.\n",
      "\n",
      "Args:\n",
      "    args (`str` or `List[str]` or `Dict[str]`, or `List[Dict[str]]`):\n",
      "        One or several texts to classify. In order to use text pairs for your classification, you can send a\n",
      "        dictionary containing `{\"text\", \"text_pair\"}` keys, or a list of those.\n",
      "    top_k (`int`, *optional*, defaults to `1`):\n",
      "        How many results to return.\n",
      "    function_to_apply (`str`, *optional*, defaults to `\"default\"`):\n",
      "        The function to apply to the model outputs in order to retrieve the scores. Accepts four different\n",
      "        values:\n",
      "\n",
      "        If this argument is not specified, then it will apply the following functions according to the number\n",
      "        of labels:\n",
      "\n",
      "        - If the model has a single label, will apply the sigmoid function on the output.\n",
      "        - If the model has several labels, will apply the softmax function on the output.\n",
      "\n",
      "        Possible values are:\n",
      "\n",
      "        - `\"sigmoid\"`: Applies the sigmoid function on the output.\n",
      "        - `\"softmax\"`: Applies the softmax function on the output.\n",
      "        - `\"none\"`: Does not apply any function on the output.\n",
      "\n",
      "Return:\n",
      "    A list or a list of list of `dict`: Each result comes as list of dictionaries with the following keys:\n",
      "\n",
      "    - **label** (`str`) -- The label predicted.\n",
      "    - **score** (`float`) -- The corresponding probability.\n",
      "\n",
      "    If `top_k` is used, one such dictionary is returned per label."
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "## the power of asking questions\n",
    "classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we tag on *two* question marks: `??`, then we get the full source code of the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           TextClassificationPipeline\n",
      "\u001b[0;31mString form:\u001b[0m    <transformers.pipelines.text_classification.TextClassificationPipeline object at 0x11cd918d0>\n",
      "\u001b[0;31mFile:\u001b[0m           ~/mambaforge/envs/llm-env/lib/python3.11/site-packages/transformers/pipelines/text_classification.py\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;34m@\u001b[0m\u001b[0madd_end_docstrings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbuild_pipeline_init_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhas_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"\u001b[0m\n",
      "\u001b[0;34m        return_all_scores (`bool`, *optional*, defaults to `False`):\u001b[0m\n",
      "\u001b[0;34m            Whether to return all prediction scores or just the one of the predicted class.\u001b[0m\n",
      "\u001b[0;34m        function_to_apply (`str`, *optional*, defaults to `\"default\"`):\u001b[0m\n",
      "\u001b[0;34m            The function to apply to the model outputs in order to retrieve the scores. Accepts four different values:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m            - `\"default\"`: if the model has a single label, will apply the sigmoid function on the output. If the model\u001b[0m\n",
      "\u001b[0;34m              has several labels, will apply the softmax function on the output.\u001b[0m\n",
      "\u001b[0;34m            - `\"sigmoid\"`: Applies the sigmoid function on the output.\u001b[0m\n",
      "\u001b[0;34m            - `\"softmax\"`: Applies the softmax function on the output.\u001b[0m\n",
      "\u001b[0;34m            - `\"none\"`: Does not apply any function on the output.\"\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0mTextClassificationPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m    Text classification pipeline using any `ModelForSequenceClassification`. See the [sequence classification\u001b[0m\n",
      "\u001b[0;34m    examples](../task_summary#sequence-classification) for more information.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Example:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m    >>> from transformers import pipeline\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> classifier = pipeline(model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\u001b[0m\n",
      "\u001b[0;34m    >>> classifier(\"This movie is disgustingly good !\")\u001b[0m\n",
      "\u001b[0;34m    [{'label': 'POSITIVE', 'score': 1.0}]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> classifier(\"Director tried too much.\")\u001b[0m\n",
      "\u001b[0;34m    [{'label': 'NEGATIVE', 'score': 0.996}]\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This text classification pipeline can currently be loaded from [`pipeline`] using the following task identifier:\u001b[0m\n",
      "\u001b[0;34m    `\"sentiment-analysis\"` (for classifying sequences according to positive or negative sentiments).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    If multiple classification labels are available (`model.config.num_labels >= 2`), the pipeline will run a softmax\u001b[0m\n",
      "\u001b[0;34m    over the results. If there is a single label, the pipeline will run a sigmoid over the result.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The models that this pipeline can use are models that have been fine-tuned on a sequence classification task. See\u001b[0m\n",
      "\u001b[0;34m    the up-to-date list of available models on\u001b[0m\n",
      "\u001b[0;34m    [huggingface.co/models](https://huggingface.co/models?filter=text-classification).\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_all_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_model_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mTF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m \u001b[0mMODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_sanitize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_all_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Using \"\" as default argument because we're going to use `top_k=None` in user code to declare\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# \"No top_k\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpreprocess_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpostprocess_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"return_all_scores\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreturn_all_scores\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mreturn_all_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_all_scores\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_legacy\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mreturn_all_scores\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"`return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\" `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mUserWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mreturn_all_scores\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationFunction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mfunction_to_apply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"function_to_apply\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Classify the text(s) given as inputs.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            args (`str` or `List[str]` or `Dict[str]`, or `List[Dict[str]]`):\u001b[0m\n",
      "\u001b[0;34m                One or several texts to classify. In order to use text pairs for your classification, you can send a\u001b[0m\n",
      "\u001b[0;34m                dictionary containing `{\"text\", \"text_pair\"}` keys, or a list of those.\u001b[0m\n",
      "\u001b[0;34m            top_k (`int`, *optional*, defaults to `1`):\u001b[0m\n",
      "\u001b[0;34m                How many results to return.\u001b[0m\n",
      "\u001b[0;34m            function_to_apply (`str`, *optional*, defaults to `\"default\"`):\u001b[0m\n",
      "\u001b[0;34m                The function to apply to the model outputs in order to retrieve the scores. Accepts four different\u001b[0m\n",
      "\u001b[0;34m                values:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m                If this argument is not specified, then it will apply the following functions according to the number\u001b[0m\n",
      "\u001b[0;34m                of labels:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m                - If the model has a single label, will apply the sigmoid function on the output.\u001b[0m\n",
      "\u001b[0;34m                - If the model has several labels, will apply the softmax function on the output.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m                Possible values are:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m                - `\"sigmoid\"`: Applies the sigmoid function on the output.\u001b[0m\n",
      "\u001b[0;34m                - `\"softmax\"`: Applies the softmax function on the output.\u001b[0m\n",
      "\u001b[0;34m                - `\"none\"`: Does not apply any function on the output.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Return:\u001b[0m\n",
      "\u001b[0;34m            A list or a list of list of `dict`: Each result comes as list of dictionaries with the following keys:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m            - **label** (`str`) -- The label predicted.\u001b[0m\n",
      "\u001b[0;34m            - **score** (`float`) -- The corresponding probability.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m            If `top_k` is used, one such dictionary is returned per label.\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0m_legacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"top_k\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_legacy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# This pipeline is odd, and return a list when single item is run\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenericTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# It used to be valid to use a list of list of list for text pairs, keeping this path for BC\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# This is likely an invalid usage of the pipeline attempting to pass text pairs.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"The pipeline received invalid inputs, if you are trying to send text pairs, you can try to send a\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m' dictionary `{\"text\": \"My text\", \"text_pair\": \"My pair\"}` in order to send a text pair.'\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# `XXXForSequenceClassification` models should not use `use_cache=True` even if it's supported\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;34m\"use_cache\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_cache\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_legacy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# `_legacy` is used to determine if we're running the naked pipeline and in backward\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# compatibility mode, or if running the pipeline with `pipeline(..., top_k=1)` we're running\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# the more natural result containing the list.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Default value before `set_parameters`\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mfunction_to_apply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi_label_classification\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGMOID\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single_label_classification\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOFTMAX\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"function_to_apply\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunction_to_apply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mClassificationFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGMOID\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mClassificationFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOFTMAX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mClassificationFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unrecognized `function_to_apply` argument: {function_to_apply}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_legacy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"score\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdict_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m{\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"score\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_legacy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdict_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mdict_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mdict_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCall docstring:\u001b[0m\n",
      "Classify the text(s) given as inputs.\n",
      "\n",
      "Args:\n",
      "    args (`str` or `List[str]` or `Dict[str]`, or `List[Dict[str]]`):\n",
      "        One or several texts to classify. In order to use text pairs for your classification, you can send a\n",
      "        dictionary containing `{\"text\", \"text_pair\"}` keys, or a list of those.\n",
      "    top_k (`int`, *optional*, defaults to `1`):\n",
      "        How many results to return.\n",
      "    function_to_apply (`str`, *optional*, defaults to `\"default\"`):\n",
      "        The function to apply to the model outputs in order to retrieve the scores. Accepts four different\n",
      "        values:\n",
      "\n",
      "        If this argument is not specified, then it will apply the following functions according to the number\n",
      "        of labels:\n",
      "\n",
      "        - If the model has a single label, will apply the sigmoid function on the output.\n",
      "        - If the model has several labels, will apply the softmax function on the output.\n",
      "\n",
      "        Possible values are:\n",
      "\n",
      "        - `\"sigmoid\"`: Applies the sigmoid function on the output.\n",
      "        - `\"softmax\"`: Applies the softmax function on the output.\n",
      "        - `\"none\"`: Does not apply any function on the output.\n",
      "\n",
      "Return:\n",
      "    A list or a list of list of `dict`: Each result comes as list of dictionaries with the following keys:\n",
      "\n",
      "    - **label** (`str`) -- The label predicted.\n",
      "    - **score** (`float`) -- The corresponding probability.\n",
      "\n",
      "    If `top_k` is used, one such dictionary is returned per label."
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "## really curious about classifier\n",
    "classifier??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `?` and `??` are excellent and quick ways to look under the hood of any object in Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting a specific `classifier` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the function that does the heavy lifting for our sentiment analysis task: `forward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Pipeline.forward of <transformers.pipelines.text_classification.TextClassificationPipeline object at 0x11cd918d0>>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # looking at what actually runs the inputs\n",
    "classifier.forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this function actually do? Let's find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_placement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0minference_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Framework {self.framework} is not supported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/mambaforge/envs/llm-env/lib/python3.11/site-packages/transformers/pipelines/base.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "# source code of the forward function\n",
    "classifier.forward??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it automatically handles whether we're running a TensorFlow (`tf`) or PyTorch (`pt`) model. Then, it makes sure the tensors are on the correct device. Lastly is calls another function, `_forward()` on the prepared inputs.  \n",
    "\n",
    "We can follow the rabbit hole as far down as needed. Let's take a look at the source of `_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "_forward will receive the prepared dictionary from `preprocess` and run it on the model. This method might\n",
      "involve the GPU or the CPU and should be agnostic to it. Isolating this function is the reason for `preprocess`\n",
      "and `postprocess` to exist, so that the hot path, this method generally can run as fast as possible.\n",
      "\n",
      "It is not meant to be called directly, `forward` is preferred. It is basically the same but contains additional\n",
      "code surrounding `_forward` making sure tensors and models are on the same device, disabling the training part\n",
      "of the code (leading to faster inference).\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "    \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# `XXXForSequenceClassification` models should not use `use_cache=True` even if it's supported\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;34m\"use_cache\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_cache\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/mambaforge/envs/llm-env/lib/python3.11/site-packages/transformers/pipelines/text_classification.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "# going deeper\n",
    "classifier._forward??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, we can see it calls the `model` of the classifier. This is the `distilbert` model we saw earlier! Now we can peek under the hood at the actual Transformer LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the distilbert sentiment analysis model\n",
    "classifier.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will breakdown the different pieces in this model later on in the course.  \n",
    "\n",
    "The important takeaway for now is that this shows the main structure of most Transformer LLM models. The changes are mostly incremental from this foundation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
